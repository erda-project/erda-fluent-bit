[SERVICE]
    flush        0.5
    daemon       Off
    grace        5
    log_level    ${LOG_LEVEL}
    parsers_file parsers.conf
    plugins_file plugins.conf
    http_server  On
    http_listen  0.0.0.0
    http_port    2020

[INPUT]
    name                 tail
    path                 /var/log/containers/*.log
    DB                   /data/spot/fluent-bit/flb_k8s.db
    Tag                  kube.*
    #read_from_head       ${INPUT_TAIL_READ_FROM_HEAD}
    #ignore_older         5m
    Buffer_Chunk_Size    1MB
    Buffer_Max_Size      10MB
    Skip_Long_Lines      On
    Skip_Empty_Lines     On
    Refresh_Interval     30
    Rotate_Wait          60
    DB.locking           true
    DB.journal_mode      WAL

    Mem_Buf_Limit        300MB
    multiline.parser     docker, cri

[FILTER]
    Name         throttle
    Match        *
    Rate         ${FLUENTBIT_THROTTLE_RATE}
    Window       5
    Interval     1s
    Print_Status ${FLUENTBIT_THROTTLE_PRINT_STATUS}
    Retain       ${FLUENTBIT_THROTTLE_RETAIN}

[FILTER]
    name            parser
    Match           *
    Key_Name        log
    Preserve_Key    False
    Reserve_Data    True
    Parser          truncate-log

# may crash and lost log when a high load (issue: https://github.com/fluent/fluent-bit/issues/4940)
# if enable filesystem storage may occupy huge disk space when a high load (issue: https://github.com/fluent/fluent-bit/issues/3718)
# In fact, the emitter seems don't handle the back pressure correctly.
# So I only have to increase emitter_mem_buf_limit and decrease flush_ms, hope to decrease the possibly of log lost.
[FILTER]
    Name                   multiline
    match                  kube.*
    multiline.key_content  log
    multiline.parser       java
    flush_ms               500
    emitter_mem_buf_limit  100MB

[FILTER]
    Name                kubernetes
    Match               kube.*
    Buffer_Size         5MB
    Kube_URL            ${MASTER_VIP_URL}
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
    Kube_Tag_Prefix     kube.var.log.containers.
    Merge_Log           On
    Merge_Log_Key       log_processed
    K8S-Logging.Parser  On
    K8S-Logging.Exclude Off
    Labels              Off
    Cache_Use_Docker_Id On
    # max ttl is 5 minitues
    Kube_Meta_Cache_TTL 300

[FILTER]
    Name            parser
    Match           *
    Key_Name        log
    Preserve_Key    True
    Reserve_Data    True
    Parser          erda-info

# kubernetes related
[FILTER]
    name           nest
    match          kube.*
    operation      lift
    nested_under   kubernetes
    add_prefix     k8s_

[FILTER]
    name nest
    match kube.*
    operation lift
    nested_under k8s_annotations
    add_prefix k8s_anno_

[FILTER]
    name nest
    match kube.*
    operation nest
    wildcard k8s_anno_msp.erda.cloud/*
    # metadata from erda platform
    nest_under tags_platform
    remove_prefix k8s_anno_msp.erda.cloud/

[FILTER]
    name modify
    match kube.*
    remove_wildcard k8s_anno_

[FILTER]
    name nest
    match kube.*
    operation nest
    wildcard k8s_*
    # metadata from k8s pod info
    nest_under tags_k8s
    remove_prefix k8s_

[FILTER]
    name nest
    match kube.*
    operation lift
    nested_under tags_parser
    add_prefix __tags_
[FILTER]
    name nest
    match kube.*
    operation lift
    nested_under tags_platform
    add_prefix __tags_
[FILTER]
    name nest
    match kube.*
    operation lift
    nested_under tags_k8s
    add_prefix __tags_

# add default tag
[FILTER]
    name modify
    match *
    add __tags_cluster_name ${DICE_CLUSTER_NAME}

# --- compatibility format block ---
# compatibility keyword
[FILTER]
    name modify
    match *
    add source container
    rename log content
    rename __tags_docker_id __tags_container_id
    rename __tags_namespace_name __tags_pod_namespace
    copy __tags_container_id id
    remove __tags_container_hash
    remove __tags_container_image
    # for routing pipeline job's logs
    rename __tags_terminus_define_tag __pri_terminus_define_tag

# compatibility dice_ prefix
[FILTER]
    name modify
    match *
    copy __tags_runtime_id __tags_dice_runtime_id
    copy __tags_runtime_name __tags_dice_runtime_name
    copy __tags_application_id __tags_dice_application_id
    copy __tags_application_name __tags_dice_application_name
    copy __tags_service_name __tags_dice_service_name
    copy __tags_workspace __tags_dice_workspace
    copy __tags_project_id __tags_dice_project_id
    copy __tags_project_name __tags_dice_project_name
    copy __tags_org_id __tags_dice_org_id
    copy __tags_org_name __tags_dice_org_name
    copy __tags_cluster_name __tags_dice_cluster_name

# compatibility log exporter
[FILTER]
    name nest
    match *
    operation nest
    wildcard __tags_monitor_log_*
    nest_under labels
    remove_prefix __tags_

[FILTER]
    name nest
    match *
    operation nest
    wildcard __tags_*
    nest_under tags
    remove_prefix __tags_
# --- compatibility format block ---

# routing log which will be send to log analysis&log exporter
[FILTER]
    name rewrite_tag
    match kube.*
    Rule $labels['monitor_log_collector'] .+ erda.log.export true
    Emitter_Name erda_log_export

# routing pipeline job's log
[FILTER]
    name rewrite_tag
    match kube.*
    Rule $__pri_terminus_define_tag .+ erda.log.job false
    Emitter_Name erda_log_job

[FILTER]
    name modify
    match  erda.log.job
    hard_copy __pri_terminus_define_tag id
    set source job

# remove private kv
[FILTER]
    name modify
    match *
    remove_wildcard __pri_

[OUTPUT]
    name                http
    Match               kube.*
    host                ${COLLECTOR_HOST}
    port                ${COLLECTOR_PORT}
    uri                 /collect/logs/container
    compress            gzip
    format              json
    http_user           ${COLLECTOR_AUTH_USERNAME}
    http_passwd         ${COLLECTOR_AUTH_PASSWORD}
    json_date_key       false
    json_date_format    iso8601
    Retry_Limit         1000
    log_response_payload false
    tls                  ${OUTPUT_HTTP_TLS}

[OUTPUT]
    name                http
    Match               erda.log.job
    host                ${COLLECTOR_HOST}
    port                ${COLLECTOR_PORT}
    uri                 /collect/logs/job
    compress            gzip
    format              json
    http_user           ${COLLECTOR_AUTH_USERNAME}
    http_passwd         ${COLLECTOR_AUTH_PASSWORD}
    json_date_key       false
    json_date_format    iso8601
    Retry_Limit         100
    log_response_payload false
    tls                  ${OUTPUT_HTTP_TLS}

[OUTPUT]
    name                erda
    Match               erda.log.export
    basic_auth_username ${COLLECTOR_AUTH_USERNAME}
    basic_auth_password ${COLLECTOR_AUTH_PASSWORD}
